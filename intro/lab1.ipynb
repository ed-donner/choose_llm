{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c18455-71ac-4bc7-8568-5f77c4b60ac1",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "# STRAIGHT TO ACTION!\n",
    "\n",
    "Welcome to our first Jupyter Lab where we will see rapid, satisfying results!\n",
    "\n",
    "I will leave with you to try out leading LLMs through their Chat Interfaces\n",
    "\n",
    "Together, we will call them using their APIs\n",
    "\n",
    "Please see the README for instructions on setting this up and getting your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810279d-0b70-457d-8713-d9023a6650bf",
   "metadata": {},
   "source": [
    "# If this is your first time in a Jupyter Notebook..\n",
    "\n",
    "Welcome to the world of Data Science experimentation. Warning: Jupyter Notebooks are very addictive and you may find it hard to go back to IDEs afterwards!!\n",
    "\n",
    "Simply click in each cell with code and press `Shift + Enter` to execute the code and print the results.\n",
    "\n",
    "There's a notebook called \"Guide to Jupyter\" in the parent directory that will give you a handy tutorial on all things Jupyter Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f2064-946b-4445-bd9e-44aa58c45e29",
   "metadata": {},
   "source": [
    "## Part 1: For you to experiment: Frontier models through their Chat UI\n",
    "\n",
    "The way that you are probably most familiar working with leading LLMs: through their tools.  \n",
    "Some questions you can try asking them:\n",
    "1. What kinds of business problem are most suitable for an LLM solution?\n",
    "2. How many words are there in your answer to this prompt?\n",
    "3. How many rainbows does it take to jump from Hawaii to seventeen?\n",
    "4. What does it feel like to be jealous?\n",
    "\n",
    "**ChatGPT** from OpenAI needs no introduction.\n",
    "\n",
    "Let's try some hard questions, and use the new o1 model as well as GPT-4o. Also try GPT-4o with canvas.\n",
    "\n",
    "https://chatgpt.com/?model=gpt-4o\n",
    "\n",
    "**Claude** from Anthropic is favored by many data scientists, with focus on safety, personality and brevity.\n",
    "\n",
    "https://claude.ai/new\n",
    "\n",
    "**Gemini** from Google is becoming increasingly well known as its results are surfaced in Google searches.\n",
    "\n",
    "https://gemini.google.com/app\n",
    "\n",
    "**Command R+** from Cohere focuses on accuracy and makes extensive use of RAG\n",
    "\n",
    "https://coral.cohere.com/\n",
    "\n",
    "**Meta AI** from Meta is their chat UI on their famous Llama open-source model\n",
    "\n",
    "https://www.meta.ai/\n",
    "\n",
    "**Perplexity** from Perplexity is a Search Engine well known for its customized search results\n",
    "\n",
    "https://www.perplexity.ai/\n",
    "\n",
    "**LeChat** from Mistral is the Web UI from the French AI powerhouse\n",
    "\n",
    "https://chat.mistral.ai/\n",
    "\n",
    "**DeepSeek** from DeepSeek AI needs no introduction! Deepseek-R1 is the Reasoning model, V3 is their Chat model.\n",
    "\n",
    "https://chat.deepseek.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058d6a3-558f-4e12-aff6-5fa949ecd454",
   "metadata": {},
   "source": [
    "## Conclusions and Takeways from exploring the Chat UIs\n",
    "\n",
    "- These models are astonishing\n",
    "- Reasoning vs Chat models - different capabilities and use cases\n",
    "- Price is highly competitive\n",
    "\n",
    "You'll find cost and other comparisons at this very useful leaderboard:\n",
    "\n",
    "https://www.vellum.ai/llm-leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24228179-fe9e-4098-8669-6898459eaa76",
   "metadata": {},
   "source": [
    "## PART 2: Calling Frontier Models through APIs\n",
    "\n",
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you'll need to create API keys from OpenAI, Anthropic and Google, and also DeepSeek and Groq if you wish.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables.\n",
    "\n",
    "EITHER (recommended) create a file called `.env` in this project root directory, and set your keys there:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "OR enter the keys directly in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1369b-8029-4607-bfab-e228aa12dec2",
   "metadata": {},
   "source": [
    "## Two purposes of these APIs:\n",
    "\n",
    "1. Illustrate the different uses of APIs\n",
    "2. Experiment with some LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3506e-b46f-44f0-ba9b-6b002835d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae0a99-0235-4256-9874-1b5e718b6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c6483-a9a6-4e3b-b361-e7a8c7f912e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google\n",
    "# All 3 APIs are similar\n",
    "# Having problems with API files? You can use openai = OpenAI(api_key=\"your-key-here\") and same for claude\n",
    "# Having problems with Google Gemini setup? Then just skip Gemini; you'll get all the experience you need from GPT and Claude.\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()\n",
    "\n",
    "# DeepSeek uses the OpenAI python client\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584134a-68c6-4677-b4b0-1a7c1c93aeb4",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A **system message** that gives overall context for the role the LLM is playing\n",
    "- A **user message** that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic.\n",
    "\n",
    "### The standard format of messages with an LLM, first used by OpenAI in its API and now adopted more widely\n",
    "\n",
    "Conversations use this format:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbcc5b-8517-4a3e-9ca1-e860a072bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke that's related to Large Language Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead0d09-cf0f-443c-b2ce-ff1a4ecdf49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d54df-4b6c-4550-86f9-a3773ccca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"How would you describe the color blue to someone who has never been able to see, in no more than 3 sentences.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cd59d-3efe-41e7-b149-ce56b9697a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=tell_a_joke\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956fbe1-0510-4769-927e-4150dcbbcfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=tell_a_joke,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b64f8-86e2-4cc1-9cd1-03ac53287f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o1\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model='o1-pro',\n",
    "    messages=[tell_a_joke[0]],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbb3fe-6c9e-4f1a-ae16-c20445fc5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543cc49-fe01-4a6c-af4e-c576f4bbc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API for Gemini has a slightly different structure - this is using Gemini 2.0 Flash\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e79468-3e70-4382-9072-726f5150808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseek-V3\n",
    "\n",
    "response = deepseek.chat.completions.create(\n",
    "    model='deepseek-chat',\n",
    "    messages=tell_a_joke\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4378a4-40ca-47bc-ac9f-dca175e49cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseek-R1\n",
    "\n",
    "response = deepseek.chat.completions.create(\n",
    "    model='deepseek-reasoner',\n",
    "    messages=tell_a_joke\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e4982-c3af-48fd-8ce9-b44a86898800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with a proper question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a knowledgable assistant, and you respond in markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08a70a-f0d8-4317-8690-c12933e0d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7a0e3-785b-403d-91a1-d28ea8297943",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08b981-f27e-4e50-aaa7-6ab1c0e09f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557a7c8-b574-4394-a6ca-97d49a45d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b352f82-a94e-43b5-a4c7-71c6eeb5d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(call_gpt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9cc43-e9b7-4179-b318-1e60998665bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23ebf0-2816-4953-94ec-a96b931179d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16d1f1-ea9c-45c2-9228-7f63d1b00a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c88f8-9b16-475e-9f98-a3c7a2a1caed",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "This was an entertaining exercise!\n",
    "\n",
    "At the same time, it hopefully gave you some perspective on:\n",
    "- The use of system prompts to set tone and character\n",
    "- The way that the entire conversation history is passed in to each API call, giving the illusion that LLMs have memory of the chat so far\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Try different characters; try swapping Claude with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2b8e3-182b-430b-9c89-d5e89f32a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And just to show you how easy it is: let's generate an image\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import base64\n",
    "\n",
    "response = openai.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=f\"A photorealistic 3d image that illustrates someone choosing between a huge number of Large Language Models\",\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",\n",
    "  n=1,\n",
    "  response_format=\"b64_json\"\n",
    ")\n",
    "\n",
    "# Extract the image data and display it\n",
    "image_base64 = response.data[0].b64_json\n",
    "image_data = base64.b64decode(image_base64)\n",
    "display(Image(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ead89-a107-429a-be0b-c61eb7264129",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=f\"A vibrant, pop-art style image that illustrates someone choosing between a huge number of Large Language Models\",\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",\n",
    "  n=1,\n",
    "  response_format=\"b64_json\"\n",
    ")\n",
    "\n",
    "# Extract the image data and display it\n",
    "image_base64 = response.data[0].b64_json\n",
    "image_data = base64.b64decode(image_base64)\n",
    "display(Image(image_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4f471-d5e0-4631-9cf1-520d1dc6e4da",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "## Recap: first we tried Frontier LLMs through their chat interfaces\n",
    "## Then we called Cloud APIs\n",
    "## Now try the 3rd way to use LLMs - direct inference of Open Source Models running locally with Ollama\n",
    "\n",
    "Visit the README for instructions on installing Ollama locally.\n",
    "\n",
    "You can see some comparisons of Open Source models on the [HuggingFace OpenLLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?params=-1%2C15&types=continuously+pretrained%2Cpretrained&filters=is_merged&official=true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06f0be-fcfc-4158-97d7-957bf36156ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2\n",
    "!ollama pull gemma2\n",
    "!ollama pull qwen2.5\n",
    "!ollama pull phi4\n",
    "!ollama pull deepseek-r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b54949-1127-4063-b723-de95bbd116a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa7dae-4c63-4adb-a043-137f0abd25f4",
   "metadata": {},
   "source": [
    "# Let's ask these 4 open-source models to tell a joke\n",
    "\n",
    "## These are examples of \"SLMs\", or Small Language Models, but they pack a punch.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32513629-2964-4648-b9c8-eb4db9131ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=\"llama3.2\", messages=tell_a_joke)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74161fd-ab3c-4658-94f5-87871b668b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=\"gemma2:2b\", messages=tell_a_joke)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabaf575-566e-462b-8883-01f808efaa4d",
   "metadata": {},
   "source": [
    "# You can also use the OpenAI Client Libraries to connect with Ollama locally\n",
    "\n",
    "Let's stream back a joke from some big models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e64867-2b90-4855-ac05-99aecb8f4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "stream = ollama_via_openai.chat.completions.create(\n",
    "    model=\"qwen2.5:14b\",\n",
    "    messages=tell_a_joke,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c0335-2dcd-4d27-93fd-deee11444884",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama_via_openai.chat.completions.create(\n",
    "    model=\"phi4\",\n",
    "    messages=tell_a_joke,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0fb7b-f4f6-497d-8ac4-66230c228e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1\",\n",
    "    messages=tell_a_joke,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5274ec2-4a42-431b-b4a1-e2874bc23c05",
   "metadata": {},
   "source": [
    "## There are also Cloud API services like we used in Part 2 that allow us to run open-source models..\n",
    "\n",
    "## Like the blazingly fast inference service Groq\n",
    "\n",
    "We can also use the amazing inference platform Groq to run inference on large open-source models that would not fit on a local machine.\n",
    "\n",
    "You'd need to create an account and API key at https://console.groq.com and then add it to your .env file:\n",
    "\n",
    "`GROQ_API_KEY=gsk_...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6208b8a-1635-4409-a0bd-db950b1e9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "groq = Groq()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513e4d3-9130-41c1-8f14-e0ff9a6d4d70",
   "metadata": {},
   "source": [
    "## Let's start with the massive Mixture Of Experts model Mixtral\n",
    "\n",
    "This could take a long time as this is a huge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6428f7e5-d3b0-4c3f-bd18-734ef38afad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = groq.chat.completions.create(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    messages=prompts,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d560eb-3e9c-4495-b06a-6f0dbb8f448b",
   "metadata": {},
   "source": [
    "## And now for the DeepSeek-r1 model distilled to 70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d0bfb-a95f-4347-9ca1-ffeda8c12a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = groq.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",  # deepseek-r1-distill-llama-70b\n",
    "    messages=tell_a_joke,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5c512-a963-4f3f-b317-2de61459eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
