{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c18455-71ac-4bc7-8568-5f77c4b60ac1",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "# STRAIGHT TO ACTION!\n",
    "\n",
    "Welcome to our first Lab where we will see rapid, satisfying results!\n",
    "\n",
    "I will leave with you to try out leading LLMs through their Chat Interfaces\n",
    "\n",
    "Together, we will call them using their APIs\n",
    "\n",
    "Please see the README for instructions on setting this up and getting your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810279d-0b70-457d-8713-d9023a6650bf",
   "metadata": {},
   "source": [
    "# If this is your first time in a Jupyter Notebook..\n",
    "\n",
    "Welcome to the world of Data Science experimentation. Warning: Jupyter Notebooks are very addictive and you may find it hard to go back to IDEs afterwards!!\n",
    "\n",
    "Simply click in each cell with code and press `Shift + Enter` to execute the code and print the results.\n",
    "\n",
    "There's a notebook called \"Guide to Jupyter\" in the parent directory that will give you a handy tutorial on all things Jupyter Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24228179-fe9e-4098-8669-6898459eaa76",
   "metadata": {},
   "source": [
    "## First: Calling Frontier Models through APIs\n",
    "\n",
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you'll need to create API keys from OpenAI, Anthropic and Google, and also DeepSeek and Groq if you wish.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables.\n",
    "\n",
    "EITHER (recommended) create a file called `.env` in this project root directory, and set your keys there:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "OR enter the keys directly in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1369b-8029-4607-bfab-e228aa12dec2",
   "metadata": {},
   "source": [
    "## Two purposes of these APIs:\n",
    "\n",
    "1. Illustrate the different APIs\n",
    "2. Experiment with some LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3506e-b46f-44f0-ba9b-6b002835d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae0a99-0235-4256-9874-1b5e718b6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebce183-62bf-4201-8d16-4db002e08876",
   "metadata": {},
   "source": [
    "## Connecting to Python Client libraries\n",
    "\n",
    "We call Cloud APIs by making REST calls to an HTTP endpoint, passing in our keys.\n",
    "\n",
    "For convenience, OpenAI has provided a lightweight python client library that makes the HTTP calls for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c6483-a9a6-4e3b-b361-e7a8c7f912e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrappes around calls to REST endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584134a-68c6-4677-b4b0-1a7c1c93aeb4",
   "metadata": {},
   "source": [
    "## Asking LLMs a hard question that will put them to the test and illustrate their power\n",
    "\n",
    "We will come up with a challenging question to test out model performance with language and nuance.\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A **system message** that gives overall context for the role the LLM is playing\n",
    "- A **user message** that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic.\n",
    "\n",
    "### The standard format of messages with an LLM, first used by OpenAI in its API and now adopted more widely\n",
    "\n",
    "Conversations use this format:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a8cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbcc5b-8517-4a3e-9ca1-e860a072bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hardest question I could come up with\n",
    "\n",
    "system_message = \"You explain concepts concisely with powerful analogies\"\n",
    "\n",
    "user_prompt = \"In 1 short sentence, describe a rainbow to someone who's never been able to see. \\\n",
    "Then in 1 short sentence, describe the imaginary number i to someone who doesn't understand math. \\\n",
    "Then in 1 short sentence, find a connection between rainbows and imaginary numbers. \\\n",
    "Then end by stating how many words are in your answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead0d09-cf0f-443c-b2ce-ff1a4ecdf49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db01c3a5-6b2d-4ada-a1c5-d10e7993bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25274e64-580b-4684-82dd-7f3253d9154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new gpt-5-nano with new reasoning_effort=\"minimal\" setting\n",
    "\n",
    "model_name = \"gpt-5-nano\"\n",
    "response = openai.chat.completions.create(model=model_name, messages=challenge, reasoning_effort=\"minimal\")\n",
    "reply = response.choices[0].message.content\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3df74c-bb00-4d0b-954b-144f059ad09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d54df-4b6c-4550-86f9-a3773ccca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(model, stream):\n",
    "    prefix = f\"### Response from {model}:\\n\\n\"\n",
    "    reply = \"\"\n",
    "    display_handle = display(Markdown(prefix), display_id=True)\n",
    "    for chunk in stream:\n",
    "        reply += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(prefix+reply), display_id=display_handle.display_id)\n",
    "    words = reply.split('</think>')[1] if '</think>' in reply else reply\n",
    "    reply += f\"\\n\\n#### Calculated true word count: {len(words.split())}\"\n",
    "    update_display(Markdown(prefix+reply), display_id=display_handle.display_id)\n",
    "    \n",
    "    models.append(model)\n",
    "    answers.append(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204aa1f-aef2-490e-8a68-7fca90ce40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-5-mini\"\n",
    "\n",
    "stream = openai.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd2ee1-8b06-44a2-b4fe-164629d4c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-5-nano\n",
    "\n",
    "model_name = \"gpt-5-nano\"\n",
    "\n",
    "stream = openai.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956fbe1-0510-4769-927e-4150dcbbcfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-5\n",
    "\n",
    "model_name = \"gpt-5\"\n",
    "\n",
    "stream = openai.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbb3fe-6c9e-4f1a-ae16-c20445fc5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4.5 Sonnet\n",
    "\n",
    "model_name = \"claude-sonnet-4-5\"\n",
    "\n",
    "stream = anthropic.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4.5 Haiku\n",
    "\n",
    "model_name = \"claude-haiku-4-5\"\n",
    "\n",
    "stream = anthropic.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543cc49-fe01-4a6c-af4e-c576f4bbc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 2.5 Flash\n",
    "\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "stream = gemini.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204e1bd-44c0-43c7-9ebc-05a366377050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 2.5 Pro\n",
    "\n",
    "model_name = \"gemini-2.5-pro\"\n",
    "\n",
    "stream = gemini.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e79468-3e70-4382-9072-726f5150808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseek-V3\n",
    "\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "stream = deepseek.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4378a4-40ca-47bc-ac9f-dca175e49cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseek-R1\n",
    "# This takes too long! It can get stuck in a loop \n",
    "\n",
    "# model_name = \"deepseek-reasoner\"\n",
    "\n",
    "# response = deepseek.chat.completions.create(model=model_name, messages=challenge)\n",
    "# reply = response.choices[0].message.content\n",
    "\n",
    "# record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d95fe-2ea5-43d9-bc3a-109c75d6058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grok from x.ai\n",
    "\n",
    "model_name = \"grok-4\"\n",
    "\n",
    "stream = grok.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db74d2f-8dfe-402f-9e74-a1c1d2a6fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq - OpenAI OSS 120B\n",
    "\n",
    "model_name = \"openai/gpt-oss-120b\"\n",
    "\n",
    "stream = groq.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b629b7b-01d6-46e9-bd9f-b5c1ec563419",
   "metadata": {},
   "source": [
    "# Now for local models\n",
    "\n",
    "### First we called models in the cloud\n",
    "\n",
    "### And now:\n",
    "\n",
    "Now try direct inference of Open Source Models running locally with Ollama\n",
    "Visit the README for instructions on installing Ollama locally.\n",
    "\n",
    "You can see some comparisons of Open Source models on the HuggingFace OpenLLM Leaderboard.\n",
    "\n",
    "Ollama provides an OpenAI-style local endpoint, so this will look very similar to part 2!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d6f8c-80e8-4621-90f7-6aebd8c5705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull gemma3:270m\n",
    "!ollama pull gpt-oss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bdecb7-84e8-44a8-ae7a-d29d04f445fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_url = 'http://localhost:11434/v1'\n",
    "ollama = OpenAI(base_url=ollama_url, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a52713-6b62-4664-8c53-071985c4373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84670122-33e9-412f-9ebc-924e8db9f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama3.2\n",
    "\n",
    "model_name = \"gemma3:270m\"\n",
    "\n",
    "stream = ollama.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da34ce-e7db-4a5b-a212-94c4c03b4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI OSS - the 20B version, not 120B, running on my computer!!\n",
    "\n",
    "model_name = \"gpt-oss:20b\"\n",
    "\n",
    "stream = ollama.chat.completions.create(model=model_name, messages=challenge, stream=True)\n",
    "record(model_name, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f23de4-f3e9-4433-880d-b43ef27fae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(len(models))\n",
    "print(models)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9f084-e0b7-4ce6-9cbd-d0cf49ddb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b2325-1726-458d-b7de-1c752a3943a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(together))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb6ca2-ffa3-451f-9fe3-436e868189a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(models)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{challenge[1][\"content\"]}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument and accuracy of word count, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor; after each response, the actual word count of their answer is given:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3854fb-8c74-4c95-a158-3776944d8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(judge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4dc61-0ed0-4fd1-bf26-f06c0de0ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9d9e4-e9f1-4bce-ad5a-6212f9310d0f",
   "metadata": {},
   "source": [
    "# Not very scientific - but quite interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ecb8cb-3d05-4cd7-a298-a519d289ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\",messages=judge_messages)\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8c7d0-0e8a-4168-a95e-f566eec09ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = models[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7a0e3-785b-403d-91a1-d28ea8297943",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08b981-f27e-4e50-aaa7-6ab1c0e09f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-nano\"\n",
    "claude_model = \"claude-haiku-4-5\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557a7c8-b574-4394-a6ca-97d49a45d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b352f82-a94e-43b5-a4c7-71c6eeb5d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(call_gpt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9cc43-e9b7-4179-b318-1e60998665bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23ebf0-2816-4953-94ec-a96b931179d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16d1f1-ea9c-45c2-9228-7f63d1b00a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c88f8-9b16-475e-9f98-a3c7a2a1caed",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "This was an entertaining exercise!\n",
    "\n",
    "At the same time, it hopefully gave you some perspective on:\n",
    "- The use of system prompts to set tone and character\n",
    "- The way that the entire conversation history is passed in to each API call, giving the illusion that LLMs have memory of the chat so far\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Try different characters; try swapping Claude with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5c512-a963-4f3f-b317-2de61459eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
