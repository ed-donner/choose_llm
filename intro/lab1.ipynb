{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c18455-71ac-4bc7-8568-5f77c4b60ac1",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "# STRAIGHT TO ACTION!\n",
    "\n",
    "Welcome to our first Jupyter Lab where we will see rapid, satisfying results!\n",
    "\n",
    "I will leave with you to try out leading LLMs through their Chat Interfaces\n",
    "\n",
    "Together, we will call them using their APIs\n",
    "\n",
    "Please see the README for instructions on setting this up and getting your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810279d-0b70-457d-8713-d9023a6650bf",
   "metadata": {},
   "source": [
    "# If this is your first time in a Jupyter Notebook..\n",
    "\n",
    "Welcome to the world of Data Science experimentation. Warning: Jupyter Notebooks are very addictive and you may find it hard to go back to IDEs afterwards!!\n",
    "\n",
    "Simply click in each cell with code and press `Shift + Enter` to execute the code and print the results.\n",
    "\n",
    "There's a notebook called \"Guide to Jupyter\" in the parent directory that will give you a handy tutorial on all things Jupyter Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f2064-946b-4445-bd9e-44aa58c45e29",
   "metadata": {},
   "source": [
    "## Part 1: For you to experiment: Frontier models through their Chat UI\n",
    "\n",
    "The way that you are probably most familiar working with leading LLMs: through their tools.  \n",
    "Some questions you can try asking them:\n",
    "1. What kinds of business problem are most suitable for an LLM solution?\n",
    "2. How many words are there in your answer to this prompt?\n",
    "3. How many rainbows does it take to jump from Hawaii to seventeen?\n",
    "4. What does it feel like to be jealous?\n",
    "\n",
    "**ChatGPT** from OpenAI needs no introduction.\n",
    "\n",
    "Let's try some hard questions, and use the new o1 model as well as GPT-4o. Also try GPT-4o with canvas.\n",
    "\n",
    "https://chatgpt.com/?model=gpt-4o\n",
    "\n",
    "**Claude** from Anthropic is favored by many data scientists, with focus on safety, personality and brevity.\n",
    "\n",
    "https://claude.ai/new\n",
    "\n",
    "**Gemini** from Google is becoming increasingly well known as its results are surfaced in Google searches.\n",
    "\n",
    "https://gemini.google.com/app\n",
    "\n",
    "**Command R+** from Cohere focuses on accuracy and makes extensive use of RAG\n",
    "\n",
    "https://coral.cohere.com/\n",
    "\n",
    "**Meta AI** from Meta is their chat UI on their famous Llama open-source model\n",
    "\n",
    "https://www.meta.ai/\n",
    "\n",
    "**Perplexity** from Perplexity is a Search Engine well known for its customized search results\n",
    "\n",
    "https://www.perplexity.ai/\n",
    "\n",
    "**LeChat** from Mistral is the Web UI from the French AI powerhouse\n",
    "\n",
    "https://chat.mistral.ai/\n",
    "\n",
    "**DeepSeek** from DeepSeek AI needs no introduction! Deepseek-R1 is the Reasoning model, V3 is their Chat model.\n",
    "\n",
    "https://chat.deepseek.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058d6a3-558f-4e12-aff6-5fa949ecd454",
   "metadata": {},
   "source": [
    "## Conclusions and Takeways from exploring the Chat UIs\n",
    "\n",
    "- These models are astonishing\n",
    "- Reasoning vs Chat models - different capabilities and use cases\n",
    "- Price is highly competitive\n",
    "\n",
    "You'll find cost and other comparisons at this very useful leaderboard:\n",
    "\n",
    "https://www.vellum.ai/llm-leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24228179-fe9e-4098-8669-6898459eaa76",
   "metadata": {},
   "source": [
    "## PART 2: Calling Frontier Models through APIs\n",
    "\n",
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you'll need to create API keys from OpenAI, Anthropic and Google, and also DeepSeek and Groq if you wish.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables.\n",
    "\n",
    "EITHER (recommended) create a file called `.env` in this project root directory, and set your keys there:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "OR enter the keys directly in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1369b-8029-4607-bfab-e228aa12dec2",
   "metadata": {},
   "source": [
    "## Two purposes of these APIs:\n",
    "\n",
    "1. Illustrate the different APIs\n",
    "2. Experiment with some LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3506e-b46f-44f0-ba9b-6b002835d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae0a99-0235-4256-9874-1b5e718b6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebce183-62bf-4201-8d16-4db002e08876",
   "metadata": {},
   "source": [
    "## Connecting to Python Client libraries\n",
    "\n",
    "We call Cloud APIs by making REST calls to an HTTP endpoint, passing in our keys.\n",
    "\n",
    "For convenience, the labs like OpenAI have provided lightweight python client libraries that make the HTTP calls for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c6483-a9a6-4e3b-b361-e7a8c7f912e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic client libraries\n",
    "# These are thin wrappers around calls to REST endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = Anthropic()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584134a-68c6-4677-b4b0-1a7c1c93aeb4",
   "metadata": {},
   "source": [
    "## Asking LLMs a hard question that will put them to the test and illustrate their power\n",
    "\n",
    "We will come up with a challenging question to test out model performance with language and nuance.\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A **system message** that gives overall context for the role the LLM is playing\n",
    "- A **user message** that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic.\n",
    "\n",
    "### The standard format of messages with an LLM, first used by OpenAI in its API and now adopted more widely\n",
    "\n",
    "Conversations use this format:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbcc5b-8517-4a3e-9ca1-e860a072bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hardest question I could come up with\n",
    "\n",
    "system_message = \"You are able to explain abstract concepts clearly and concisely, with powerful analogies\"\n",
    "\n",
    "user_prompt = \"In 1 sentence, describe a rainbow to someone who's never been able to see. \\\n",
    "Then in 1 sentence, describe the imaginary number i to someone who doesn't understand math. \\\n",
    "Then in 1 sentence, find a connection between rainbows and imaginary numbers. \\\n",
    "Then end by stating how many words are in your answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead0d09-cf0f-443c-b2ce-ff1a4ecdf49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d54df-4b6c-4550-86f9-a3773ccca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "answers = []\n",
    "\n",
    "def record(model, reply):\n",
    "    print(f\"Response from {model}:\\n\\n{reply}\\n\\nActual word count: {len(reply.split())}\")\n",
    "    models.append(model)\n",
    "    answers.append(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cd59d-3efe-41e7-b149-ce56b9697a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956fbe1-0510-4769-927e-4150dcbbcfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fffa000-30f5-49e3-aebb-16721d60d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.5\n",
    "\n",
    "model_name = \"gpt-4.5-preview\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b64f8-86e2-4cc1-9cd1-03ac53287f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o1\n",
    "# o1 is a \"reasoning\" model that has been trained to think through it's answer before it replies..\n",
    "\n",
    "model_name = \"o1\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9533840-b36b-49cc-bd42-4815960f3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3-mini\n",
    "\n",
    "model_name = \"o3-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbb3fe-6c9e-4f1a-ae16-c20445fc5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=model_name,\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=challenge[0][\"content\"],\n",
    "    messages=[challenge[1]]\n",
    "    )\n",
    "\n",
    "reply = message.content[0].text\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543cc49-fe01-4a6c-af4e-c576f4bbc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 2.0 Flash\n",
    "\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e79468-3e70-4382-9072-726f5150808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseek-V3\n",
    "\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4378a4-40ca-47bc-ac9f-dca175e49cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseek-R1\n",
    "# This takes too long! It can get stuck in a loop \n",
    "\n",
    "# model_name = \"deepseek-reasoner\"\n",
    "\n",
    "# response = deepseek.chat.completions.create(model=model_name, messages=challenge)\n",
    "# reply = response.choices[0].message.content\n",
    "\n",
    "# record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5a6a5-7e91-4deb-a62e-378d1949729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq - llama-3.3-70b-versatile\n",
    "\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0bfcdf-2542-43ec-8f2c-abc3b1c67dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq - deepseek-r1-distill-llama-70b\n",
    "\n",
    "model_name = \"deepseek-r1-distill-llama-70b\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "if '</think>' in reply:\n",
    "    reply = reply.split('</think>')[1]\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b629b7b-01d6-46e9-bd9f-b5c1ec563419",
   "metadata": {},
   "source": [
    "# Now for Part 3\n",
    "\n",
    "### Recap: first we tried Frontier LLMs through their chat interfaces\n",
    "\n",
    "### Then we called Cloud APIs\n",
    "\n",
    "### And now:\n",
    "\n",
    "Now try the 3rd way to use LLMs - direct inference of Open Source Models running locally with Ollama¶\n",
    "Visit the README for instructions on installing Ollama locally.\n",
    "\n",
    "You can see some comparisons of Open Source models on the HuggingFace OpenLLM Leaderboard.\n",
    "\n",
    "Ollama provides an OpenAI-style local endpoint, so this will look very similar to part 2!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d6f8c-80e8-4621-90f7-6aebd8c5705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2\n",
    "!ollama pull gemma2\n",
    "!ollama pull qwen2.5\n",
    "!ollama pull phi4\n",
    "!ollama pull deepseek-r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bdecb7-84e8-44a8-ae7a-d29d04f445fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_url = 'http://localhost:11434/v1'\n",
    "ollama = OpenAI(base_url=ollama_url, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a52713-6b62-4664-8c53-071985c4373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84670122-33e9-412f-9ebc-924e8db9f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama3.2\n",
    "\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd56e3-c856-490f-b38a-aa68a5187dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemma2\n",
    "\n",
    "model_name = \"gemma2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da34ce-e7db-4a5b-a212-94c4c03b4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen2.5\n",
    "\n",
    "model_name = \"qwen2.5\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877bbcc-02e3-4490-a3cf-bd08b8157a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi4\n",
    "\n",
    "model_name = \"phi4\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b51f11-50ee-4eab-8c3f-13b6c22386fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek-r1\n",
    "\n",
    "model_name = \"deepseek-r1\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=challenge)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "if '</think>' in reply:\n",
    "    reply = reply.split(\"</think>\")[1]\n",
    "\n",
    "record(model_name, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f23de4-f3e9-4433-880d-b43ef27fae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(models)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9f084-e0b7-4ce6-9cbd-d0cf49ddb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b2325-1726-458d-b7de-1c752a3943a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb6ca2-ffa3-451f-9fe3-436e868189a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(models)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{challenge[1][\"content\"]}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3854fb-8c74-4c95-a158-3776944d8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4dc61-0ed0-4fd1-bf26-f06c0de0ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9d9e4-e9f1-4bce-ad5a-6212f9310d0f",
   "metadata": {},
   "source": [
    "# Not very scientific - but quite interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ddf64-e35d-4a3f-8389-8e0aa5e95ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8c7d0-0e8a-4168-a95e-f566eec09ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = models[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e4982-c3af-48fd-8ce9-b44a86898800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with a proper question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a knowledgable assistant, and you respond in markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I choose the right LLM for a task? Please respond in markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08a70a-f0d8-4317-8690-c12933e0d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7a0e3-785b-403d-91a1-d28ea8297943",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08b981-f27e-4e50-aaa7-6ab1c0e09f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557a7c8-b574-4394-a6ca-97d49a45d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b352f82-a94e-43b5-a4c7-71c6eeb5d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(call_gpt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9cc43-e9b7-4179-b318-1e60998665bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23ebf0-2816-4953-94ec-a96b931179d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16d1f1-ea9c-45c2-9228-7f63d1b00a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c88f8-9b16-475e-9f98-a3c7a2a1caed",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "This was an entertaining exercise!\n",
    "\n",
    "At the same time, it hopefully gave you some perspective on:\n",
    "- The use of system prompts to set tone and character\n",
    "- The way that the entire conversation history is passed in to each API call, giving the illusion that LLMs have memory of the chat so far\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Try different characters; try swapping Claude with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2b8e3-182b-430b-9c89-d5e89f32a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And just to show you how easy it is: let's generate an image\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import base64\n",
    "\n",
    "response = openai.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=f\"A photorealistic 3d image that illustrates someone choosing between a huge number of Large Language Models\",\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",\n",
    "  n=1,\n",
    "  response_format=\"b64_json\"\n",
    ")\n",
    "\n",
    "# Extract the image data and display it\n",
    "image_base64 = response.data[0].b64_json\n",
    "image_data = base64.b64decode(image_base64)\n",
    "display(Image(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ead89-a107-429a-be0b-c61eb7264129",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=f\"An image that illustrates someone choosing between a huge number of Large Language Models in a vibrant pop-art style, like a Liechtenstein style, with dazzling lines and colors\",\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",\n",
    "  n=1,\n",
    "  response_format=\"b64_json\"\n",
    ")\n",
    "\n",
    "# Extract the image data and display it\n",
    "image_base64 = response.data[0].b64_json\n",
    "image_data = base64.b64decode(image_base64)\n",
    "display(Image(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5c512-a963-4f3f-b317-2de61459eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
